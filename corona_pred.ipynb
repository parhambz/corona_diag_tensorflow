{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "corona_pred.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parhambz/corona_diag_tensorflow/blob/main/corona_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diagnosing coronavirus using tensorflow\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iZZXlzLMt0Sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project description\n"
      ],
      "metadata": {
        "id": "_eqjsjJN0uza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is believed by some in the future computers can work as a helping hand for the doctors especially in diagnosing diseases.Since corona virus broke out the over population of the hospitals have been a critical problem.A large number of people visit the hospitals only to be checked whether they are infected by the virus or not while there is no need to go to hospital in person.And with the advent of artificial intelligence there are many ways to separate different types of people only using the computers.In this project we try to train a model in order to facilitate the process of diagnosing coronavirus without any doctor being involved.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The project is developed using python programming language and Tensorflow which is a library written in python.\n",
        "The developed code will be explained:"
      ],
      "metadata": {
        "id": "JXtITJ0F06Ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "fs-p5Jxg0Gbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selecting the proper version of tensorflow**\n",
        "\n"
      ],
      "metadata": {
        "id": "ajCkuzbrlXz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju685Ba3UNVR",
        "outputId": "bf9c94af-4290-449f-bfd3-2556480a1894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**importing essential libraries**"
      ],
      "metadata": {
        "id": "L9KEmvJjli_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "bQL4twnTUQdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**defining the headings of the input data**\n",
        "\n",
        "The given table provides information on 8 decisive factor of recodgnizing corona which are :\n",
        "\n",
        "\n",
        "1.   Cough\n",
        "2.   Fever\n",
        "3.   Sore throat\n",
        "4.   Shortness of breath\n",
        "5.   Headache\n",
        "6.   Corona test result\n",
        "7.   Age\n",
        "8.   Gender\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "McgwDAN5lzMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_COLUMN_NAMES = ['cough', 'fever', 'sore_t', 'shortness_breath', 'headache','corona_res','age_more60','gender','test_ind']\n",
        "cov_resault = ['positive','negative']\n",
        "# Lets define some constants to help us later on\n"
      ],
      "metadata": {
        "id": "FuTpLJVrUWpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the input data**\n",
        "\n",
        "In this section provided data for train is loaded from my privately owned google drive"
      ],
      "metadata": {
        "id": "EOPoABNNoOfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/corona_tested_individuals_ver_0083.english.csv\", names=CSV_COLUMN_NAMES, header=0)\n",
        "# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEheEXfvUZZ_",
        "outputId": "594bca34-f1af-45fc-cc8d-b6f70b722659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the given data**\n",
        "\n",
        "\n",
        "*   First null values in the input data are replaced with empty values ( ' ' ).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uMJfz3qwoz4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.fillna('', inplace=True)\n"
      ],
      "metadata": {
        "id": "PlsWqS0cUbeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   In the next stage the given data table is partitioned into test data and train data by mean of the function defined below.\n",
        "\n"
      ],
      "metadata": {
        "id": "1_rbCYe8plUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_partitions_pd(df, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    # Only allows for equal validation and test splits\n",
        "    assert val_split == test_split \n",
        "\n",
        "    # Specify seed to always have the same split distribution between runs\n",
        "    df_sample = df.sample(frac=1, random_state=12)\n",
        "    indices_or_sections = [int(train_split * len(df)), int((1 - val_split - test_split) * len(df))]\n",
        "    \n",
        "    train_ds, val_ds, test_ds = np.split(df_sample, indices_or_sections)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "-49mEPiGwMFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train,val,test)=get_dataset_partitions_pd(train, train_split=0.8, val_split=0.1, test_split=0.1)"
      ],
      "metadata": {
        "id": "7Z-vy5oRwZ9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Following the preparing process in this stage the labels are extracted from the train and test data and then they are encoded (0 for negative and 1 for positive) in order to use them in training and evaluation process.\n"
      ],
      "metadata": {
        "id": "wHd4YqnzqAiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ys = train.pop('corona_res')\n",
        "train_y=[]\n",
        "for r in train_ys:\n",
        "  if (r==\"positive\"):\n",
        "    train_y.append(1)\n",
        "  else:\n",
        "    train_y.append(0)\n",
        "train_y = np.array(train_y)\n"
      ],
      "metadata": {
        "id": "N1_3IBnz6FGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ys = test.pop('corona_res')\n",
        "test_y=[]\n",
        "for r in test_ys:\n",
        "  if (r==\"positive\"):\n",
        "    test_y.append(1)\n",
        "  else:\n",
        "    test_y.append(0)\n",
        "test_y = np.array(test_y)"
      ],
      "metadata": {
        "id": "0kc5qBCJx5VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   This stage defines categorical columns in our data in order to encode them to integer values \n"
      ],
      "metadata": {
        "id": "kwHX_RAErO8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORICAL_COLUMNS = [ 'gender', 'test_ind','age_more60','cough', 'fever','sore_t','shortness_breath','headache']\n",
        "#NUMERIC_COLUMNS=['cough', 'fever','sore_t','shortness_breath','headache']\n",
        "\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  vocabulary = train[feature_name].unique()  # gets a list of all unique values from given feature column\n",
        "  feature_columns.append(tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)))\n",
        "\n",
        "#for feature_name in NUMERIC_COLUMNS:\n",
        "#  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.boolean_mask))"
      ],
      "metadata": {
        "id": "bA9IroDx331F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the input function**\n",
        "\n",
        "The defined function below is responsible of feeding the model by giving the input data to the model in a desired way."
      ],
      "metadata": {
        "id": "SYUx_buAriOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(features, labels, training=True, batch_size=256):\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    # Shuffle and repeat if you are in training mode.\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(1000).repeat()\n",
        "    \n",
        "    return dataset.batch(batch_size)\n"
      ],
      "metadata": {
        "id": "LVnHq-f8uwOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the model**\n",
        "\n",
        "In this section we create a deep neural network model to be trained to be able to classify any data."
      ],
      "metadata": {
        "id": "6tRSNR4lr9yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.q\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=feature_columns,\n",
        "    # Two hidden layers of 30 and 10 nodes respectively.\n",
        "    hidden_units=[30, 10],\n",
        "    # The model must choose between 3 classes.\n",
        "    n_classes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTGcS9UmvEUm",
        "outputId": "e6a1aa47-3ed0-4144-e624-6707f337aec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxtqqqm6b\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpxtqqqm6b', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**\n",
        "\n",
        "The training starts in this section and the data is given by the function which was defined before."
      ],
      "metadata": {
        "id": "IeXwd4c6sgCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classifier.train(\n",
        "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
        "    steps=5000)\n",
        "# We include a lambda to avoid creating an inner function previously"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBprvHLfvJLR",
        "outputId": "632881c1-a4c6-4402-ee23-33af9ec333e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adagrad.py:84: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpxtqqqm6b/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
            "INFO:tensorflow:loss = 0.8714495, step = 0\n",
            "INFO:tensorflow:global_step/sec: 138.782\n",
            "INFO:tensorflow:loss = 0.58082414, step = 100 (0.722 sec)\n",
            "INFO:tensorflow:global_step/sec: 384.394\n",
            "INFO:tensorflow:loss = 0.531592, step = 200 (0.264 sec)\n",
            "INFO:tensorflow:global_step/sec: 382.756\n",
            "INFO:tensorflow:loss = 0.44063276, step = 300 (0.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 394.883\n",
            "INFO:tensorflow:loss = 0.37942898, step = 400 (0.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 370.986\n",
            "INFO:tensorflow:loss = 0.34911203, step = 500 (0.272 sec)\n",
            "INFO:tensorflow:global_step/sec: 364.114\n",
            "INFO:tensorflow:loss = 0.31723124, step = 600 (0.274 sec)\n",
            "INFO:tensorflow:global_step/sec: 376.296\n",
            "INFO:tensorflow:loss = 0.3260751, step = 700 (0.266 sec)\n",
            "INFO:tensorflow:global_step/sec: 379.083\n",
            "INFO:tensorflow:loss = 0.42128378, step = 800 (0.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 372.142\n",
            "INFO:tensorflow:loss = 0.34764224, step = 900 (0.271 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.801\n",
            "INFO:tensorflow:loss = 0.25366536, step = 1000 (0.255 sec)\n",
            "INFO:tensorflow:global_step/sec: 393.522\n",
            "INFO:tensorflow:loss = 0.38319725, step = 1100 (0.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 376.794\n",
            "INFO:tensorflow:loss = 0.3155992, step = 1200 (0.270 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.456\n",
            "INFO:tensorflow:loss = 0.37822887, step = 1300 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 368.796\n",
            "INFO:tensorflow:loss = 0.31928012, step = 1400 (0.270 sec)\n",
            "INFO:tensorflow:global_step/sec: 371.161\n",
            "INFO:tensorflow:loss = 0.23442161, step = 1500 (0.270 sec)\n",
            "INFO:tensorflow:global_step/sec: 369.106\n",
            "INFO:tensorflow:loss = 0.3463699, step = 1600 (0.270 sec)\n",
            "INFO:tensorflow:global_step/sec: 375.469\n",
            "INFO:tensorflow:loss = 0.24234319, step = 1700 (0.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 386.588\n",
            "INFO:tensorflow:loss = 0.3019151, step = 1800 (0.257 sec)\n",
            "INFO:tensorflow:global_step/sec: 368.043\n",
            "INFO:tensorflow:loss = 0.2741059, step = 1900 (0.269 sec)\n",
            "INFO:tensorflow:global_step/sec: 371.26\n",
            "INFO:tensorflow:loss = 0.26892322, step = 2000 (0.273 sec)\n",
            "INFO:tensorflow:global_step/sec: 391.806\n",
            "INFO:tensorflow:loss = 0.30893105, step = 2100 (0.251 sec)\n",
            "INFO:tensorflow:global_step/sec: 394.938\n",
            "INFO:tensorflow:loss = 0.34379113, step = 2200 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 400.689\n",
            "INFO:tensorflow:loss = 0.2656771, step = 2300 (0.251 sec)\n",
            "INFO:tensorflow:global_step/sec: 368.715\n",
            "INFO:tensorflow:loss = 0.26418394, step = 2400 (0.271 sec)\n",
            "INFO:tensorflow:global_step/sec: 369.68\n",
            "INFO:tensorflow:loss = 0.22645046, step = 2500 (0.273 sec)\n",
            "INFO:tensorflow:global_step/sec: 391.43\n",
            "INFO:tensorflow:loss = 0.2757314, step = 2600 (0.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 379.699\n",
            "INFO:tensorflow:loss = 0.25515363, step = 2700 (0.265 sec)\n",
            "INFO:tensorflow:global_step/sec: 376.452\n",
            "INFO:tensorflow:loss = 0.32455465, step = 2800 (0.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 397.167\n",
            "INFO:tensorflow:loss = 0.22785574, step = 2900 (0.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 378.141\n",
            "INFO:tensorflow:loss = 0.28464273, step = 3000 (0.264 sec)\n",
            "INFO:tensorflow:global_step/sec: 389.587\n",
            "INFO:tensorflow:loss = 0.33077264, step = 3100 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 382.945\n",
            "INFO:tensorflow:loss = 0.23674434, step = 3200 (0.261 sec)\n",
            "INFO:tensorflow:global_step/sec: 383.117\n",
            "INFO:tensorflow:loss = 0.26189196, step = 3300 (0.261 sec)\n",
            "INFO:tensorflow:global_step/sec: 370.192\n",
            "INFO:tensorflow:loss = 0.20810834, step = 3400 (0.271 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.619\n",
            "INFO:tensorflow:loss = 0.26604086, step = 3500 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 347.737\n",
            "INFO:tensorflow:loss = 0.28943378, step = 3600 (0.288 sec)\n",
            "INFO:tensorflow:global_step/sec: 381.787\n",
            "INFO:tensorflow:loss = 0.17760395, step = 3700 (0.263 sec)\n",
            "INFO:tensorflow:global_step/sec: 372.087\n",
            "INFO:tensorflow:loss = 0.31520432, step = 3800 (0.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 379.549\n",
            "INFO:tensorflow:loss = 0.24648261, step = 3900 (0.261 sec)\n",
            "INFO:tensorflow:global_step/sec: 396.488\n",
            "INFO:tensorflow:loss = 0.26753822, step = 4000 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 370.482\n",
            "INFO:tensorflow:loss = 0.26340705, step = 4100 (0.269 sec)\n",
            "INFO:tensorflow:global_step/sec: 400.586\n",
            "INFO:tensorflow:loss = 0.32736784, step = 4200 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 376.578\n",
            "INFO:tensorflow:loss = 0.2522485, step = 4300 (0.266 sec)\n",
            "INFO:tensorflow:global_step/sec: 397.786\n",
            "INFO:tensorflow:loss = 0.25924206, step = 4400 (0.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 382.78\n",
            "INFO:tensorflow:loss = 0.26812196, step = 4500 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 385.203\n",
            "INFO:tensorflow:loss = 0.23152363, step = 4600 (0.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 373.603\n",
            "INFO:tensorflow:loss = 0.18222746, step = 4700 (0.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 370.168\n",
            "INFO:tensorflow:loss = 0.22655001, step = 4800 (0.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 393.96\n",
            "INFO:tensorflow:loss = 0.23591937, step = 4900 (0.257 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5000...\n",
            "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmpxtqqqm6b/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5000...\n",
            "INFO:tensorflow:Loss for final step: 0.25738734.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7f73caf68a50>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation code**\n",
        "\n",
        "At the end it is important to measure the accuracy of the trained model.In other words we use the separated data which was considered as a test data to calculate the accuracy and how successful our model is in classifying the infected persons from those who are not infected."
      ],
      "metadata": {
        "id": "tjdHQS9aszVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = classifier.evaluate(\n",
        "    input_fn=lambda: input_fn(test, test_y, training=False))\n",
        "\n",
        "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
      ],
      "metadata": {
        "id": "EGGucXVKvP60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02481a8a-294c-4f14-db9f-e0bc5da139fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-05-14T13:53:37\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpxtqqqm6b/model.ckpt-5000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Inference Time : 17.96255s\n",
            "INFO:tensorflow:Finished evaluation at 2022-05-14-13:53:55\n",
            "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.91906404, average_loss = 0.24883823, global_step = 5000, loss = 0.24884002\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/tmpxtqqqm6b/model.ckpt-5000\n",
            "\n",
            "Test set accuracy: 0.919\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "uuvlcnVQ1eD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the result it can be claimed developing the model was almost successful as the evaluation result shows the accuracy of about %92."
      ],
      "metadata": {
        "id": "9tFZrthY1k_7"
      }
    }
  ]
}